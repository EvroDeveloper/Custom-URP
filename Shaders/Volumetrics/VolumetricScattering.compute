///This is the realtime volumetric shader. It's the first froxel pass.

#pragma kernel Scatter

#define M_PI 3.1415926535897932384626433832795		//Standard stored Pi.
#define PI_x4 12.566370614359172953850573533118		//For inverse square.


//#include "Packages/com.unity.render-pipelines.core/ShaderLibrary/Common.hlsl"


RWTexture3D<float4> Result;
Texture3D<float4> PreviousFrameLighting;
//Texture2D<float4> BlueNoise;

Texture2DArray<float4> LightProjectionTextureArray;


//Currently only Supports spot lights
struct LightObject 
{
	float4x4    LightProjectionMatrix;
	float3      LightPosition;
	float4		LightColor;
	int			LightCookie;
};

StructuredBuffer<LightObject> LightObjects;

shared float4 CameraPosition; //current camera position
shared float4 CameraMotionVector; //camera's motion per frame
shared float4 _VolumePlaneSettings; 
shared float4 _ZBufferParams;

float4x4 inverseCameraProjectionMatrix;
shared float4x4 CameraProjectionMatrix;
float4x4 PreviousFrameMatrix;

SamplerState _LinearClamp;
SamplerState _point_repeat;
SamplerState Custom_trilinear_clamp_sampler;

shared Texture3D<float4> _VolumetricClipmapTexture;
shared float3 _ClipmapPosition;
shared float _ClipmapScale;

float3 Jittery;

//Custom trilinear interpolation
//Apparently you can't use a sampler with a RWtexture, but you can directly read the data. 
//Do we need this? Can I do this in a sampler instead without requiring a CPU memory copy? Is it any faster to do it that way? //DOES IT WORK IN ANDROID???
float4 TrilinearInterpolation(float3 UVW) {

	float3 UVW_0 = floor(UVW); //Lowest corner 
	float3 UVW_1 = ceil(UVW);	//Highest Corner

	float3 PixelDifference = UVW - UVW_0; //vec3 distance
	float3 PixelDifference_1minus = 1 - PixelDifference; 

	//Sample ALL the points!

	float4 value_000 = Result[UVW_0.xyz];
	float4 value_100 = Result[int3(UVW_1.x, UVW_0.y, UVW_0.z)];
	float4 value_010 = Result[int3(UVW_0.x, UVW_1.y, UVW_0.z)];
	float4 value_110 = Result[int3(UVW_1.x, UVW_1.y, UVW_0.z)];

	float4 value_001 = Result[int3(UVW_0.x, UVW_0.y, UVW_1.z)];
	float4 value_101 = Result[int3(UVW_1.x, UVW_0.y, UVW_1.z)];
	float4 value_011 = Result[int3(UVW_0.x, UVW_1.y, UVW_1.z)];
	float4 value_111 = Result[UVW_1.xyz];

	// Interpolate in 3 dimensions

	float4 c00 = (value_000 * (PixelDifference_1minus.x)) + (value_100 * (PixelDifference.x));
	float4 c01 = (value_001 * (PixelDifference_1minus.x)) + (value_101 * (PixelDifference.x));
	float4 c10 = (value_010 * (PixelDifference_1minus.x)) + (value_110 * (PixelDifference.x));
	float4 c11 = (value_011 * (PixelDifference_1minus.x)) + (value_111 * (PixelDifference.x));

	float4 c0 = (c00 * (PixelDifference_1minus.y)) + (c10 * (PixelDifference.y));
	float4 c1 = (c01 * (PixelDifference_1minus.y)) + (c11 * (PixelDifference.y));

	return (c0 * PixelDifference_1minus.z) + (c1 * PixelDifference.z);
};

float4 SpotLightLoop(float4 WS_coordinate) {

	float4 accumLighting = float4(0,0,0,0);

	uint count, stride;
	LightObjects.GetDimensions(count, stride);

	for (int i = 0; i < count; i++) {

		///Realtime Spotlight projection matrix and cookie sampler
		float4 lightWSPos = WS_coordinate - float4(LightObjects[i].LightPosition, 1); //world pos

		lightWSPos = mul(LightObjects[i].LightProjectionMatrix, lightWSPos);

		lightWSPos.xy = lightWSPos.xy / lightWSPos.w;

		float lightDirection = lightWSPos.z;

		lightWSPos.z = 1; //Setting which slice to sample https://docs.microsoft.com/en-us/windows/win32/direct3dhlsl/dx-graphics-hlsl-to-samplelevel

		float4 spotlightTex = LightProjectionTextureArray.SampleLevel(Custom_trilinear_clamp_sampler, lightWSPos.xyz, 0, 0);

		float LightRadius = distance(WS_coordinate.xyz, LightObjects[i].LightPosition.xyz); // Distance from light source

		spotlightTex *= LightObjects[i].LightColor / (PI_x4 *  LightRadius*LightRadius);	// inverse square

		accumLighting += spotlightTex * step(0,-lightDirection); // clip only to the front and add to the lightinbg
	}

	return accumLighting;
}


//float LinearToVaporDeviceDepth(float lin) {
//	return lin * _VaporPlaneSettings.y / (1 + lin * _VaporPlaneSettings.z);
//}

[numthreads(4,4,4)]
void Scatter(uint3 id : SV_DispatchThreadID)
{

	///Get current RT resolution and convert it to 0 - 1 UVW space
	float3 whd;
	Result.GetDimensions(whd.x, whd.y, whd.z);

	//float2 noiseDimensions;
	//BlueNoise.GetDimensions(noiseDimensions.x, noiseDimensions.y);
	//float4 Noised = BlueNoise.SampleLevel(_point_repeat, id.xy / (whd.xy / 32), 0);

	//float3 Jitter = Jittery;
	//Jitter.z = Noised.r;

	//TODO: jitter here? 
	float4 UVW = float4( (id + 0.5  ) / whd, 1); //Make uvs and sample from center of froxel //ID TO UV



	//!!!!!!!!!!!!!!!!
	//TODO: Add pow curve here

	//UVW.z = UVW.z / (_VolumePlaneSettings.y - UVW.z * _VolumePlaneSettings.z); // Converts from frustum space To Linear Depth
	UVW.z = UVW.z * _VolumePlaneSettings.y / (1 + UVW.z * _VolumePlaneSettings.z); // LinearToVaporDeviceDepth	
	//UVW.z = 1.0 / (_ZBufferParams.z * UVW.z + _ZBufferParams.w); // VaporDeviceToEyeDepth
	

	float4 accumLighting = float4(0,0,0,0);
	//	

	/// Invert the assumed perspective projection from the IDs to World Space
	float4 WS_coordinate = mul(inverseCameraProjectionMatrix, UVW); //inverse camera matrix
	WS_coordinate.xyz = WS_coordinate.xyz / WS_coordinate.w;	//tapper coord and flip around

	

	//WS_coordinate.z = WS_coordinate.z / (_VolumePlaneSettings.y - WS_coordinate.z * _VolumePlaneSettings.z); // Converts from frustum space To Linear Depth


	//WS_coordinate *= WS_coordinate;

//	WS_coordinate.z = UVW.z / 20;

	//Noised *= 2;

	//WS_coordinate.z

	//float4 PreviousFrameProjection = mul(PreviousFrameMatrix, UVW); // inverse previous frame camera matrix

	float4 PreviousFrameProjection = WS_coordinate; // Catch WS relitive to the camera

	WS_coordinate = -WS_coordinate + CameraPosition; //move to postion.
	///

//	WS_coordinate.z += Noised;


	/// Previous frame space

	PreviousFrameProjection.xyz -= CameraMotionVector.xyz; //Move back previous frame's pos
	PreviousFrameProjection.w = 1;
	PreviousFrameProjection = mul(PreviousFrameMatrix, PreviousFrameProjection); // convert using previous matrix
	PreviousFrameProjection = PreviousFrameProjection /  PreviousFrameProjection.w; //Untapper coord


	//float t0 = DecodeLogarithmicDepthGeneralized(0, _VBufferDistanceDecodingParams);
	//	float de = _VBufferRcpSliceCount; // Log-encoded distance between slices

	//float4 previousResult = TrilinearInterpolation( PreviousFrameProjection.xyz * whd.xyz - 0.5); //Custom trilinear interpolation of RW texture
	//float4 previousResult = PreviousFrameLighting.SampleLevel(_LinearClamp,  PreviousFrameProjection.xyz * whd.xyz, 0 );  //Can't use sampler! :(

	////Light loops

	//	accumLighting += SpotLightLoop(WS_coordinate);

	//Combined clipmap of baked maps. Reduces the amount of 3d texture look-ups per frame.
	//Sample based on worldspace divided by texture's world size. (apply to 0-1 Tex Sampler, no need to know the resolution)
	
	//Single debug
	//if (WS_coordinate.z < 5) {
	//	accumLighting.xyz = float3(0, 0.1, 0);
	//}
	//else accumLighting.xyz = float3(.1,0,.1);

	float3 ClipmapUVW = (WS_coordinate.xyz - (_ClipmapPosition.xyz - (_ClipmapScale *.5) )  ) / _ClipmapScale.xxx;


	//For clipping the clipmap. Isn't needed if the view volume is smaller than the clipmap
	//float Clipped =
	//	step(ClipmapUVW.x, 1) * step(0, ClipmapUVW.x) *
	//	step(ClipmapUVW.y, 1) * step(0, ClipmapUVW.y) *
	//	step(ClipmapUVW.z, 1) * step(0, ClipmapUVW.z);




	accumLighting += _VolumetricClipmapTexture.SampleLevel(_LinearClamp, ClipmapUVW, 0) ;
//	accumLighting.xyz += ClipmapUVW * Clipped;

//	ClipUVW = max(ClipUVW, 0);

	//accumLighting += ClipmapTexture.SampleLevel(_LinearClamp,UVW, 0 );
	//accumLighting += float4(1, 0, 1, 1);
//	float4 lightstuff = 1 - saturate(distance(WS_coordinate, inputPosition)); //Realtime Light things

	//Result[id.xyz] = (accumLighting * 0.4) + (previousResult * .6); // average previous frame 
	Result[id.xyz] = accumLighting ; // average previous frame 
	//Result[id.xyz] = bakedLightTex;
	//WS_coordinate.xyz -= CameraPosition.xyz;
	//WS_coordinate.xyz = -WS_coordinate.xyz * WS_coordinate.w;
	//Result[id.xyz] = mul(CameraProjectionMatrix, WS_coordinate);
	//Result[id.xyz] = frac(PreviousFrameProjection);
	//Result[id.xyz] = bakedLightTex;
	//Result[id.xyz] = PreviousFrameProjection;
	//Result[id.xyz] = WS_coordinate;
	//Result[id.xyz] = float4(id.xy / whd.xy , frac( id.z / (whd.z * .1)), 1 ) ;

}
