///This is the realtime volumetric shader. It's the first froxel pass.

#pragma kernel Scatter

#define M_PI 3.1415926535897932384626433832795		//Standard stored Pi.
#define PI_x4 12.566370614359172953850573533118		//For inverse square.

#include "Packages/com.unity.render-pipelines.core/ShaderLibrary/Common.hlsl"
//#include "Packages/com.unity.render-pipelines.core/ShaderLibrary/SpaceTransforms.hlsl"

Texture3D<float4> PreviousFrameLighting;
RWTexture3D<float4> Result;

//Texture2D<float4> BlueNoise;

Texture2DArray<float4> LightProjectionTextureArray;



//Currently only Supports spot lights
struct LightObject 
{
	float4x4    LightProjectionMatrix;
	float3      LightPosition;
	float4		LightColor;
	int			LightCookie;
};

StructuredBuffer<LightObject> LightObjects;

shared float4 CameraPosition; //current camera position
shared float4 CameraMotionVector; //camera's motion per frame
shared float4 _VolumePlaneSettings; 
shared float4 _ZBufferParams;

shared float _GlobalExtinction;

shared float4 _VBufferDistanceEncodingParams;
shared float4 _VBufferDistanceDecodingParams;
//shared float4 _VBufferSharedUvScaleAndLimit;

float4x4 inverseCameraProjectionMatrix;
shared float4x4 CameraProjectionMatrix;
shared  float4x4 TransposedCameraProjectionMatrix;

float4x4 PreviousFrameMatrix;
shared float4x4 _PrevViewProjMatrix;

shared float4x4 _VBufferCoordToViewDirWS; // >_>

SamplerState _LinearClamp;
SamplerState _point_repeat;
SamplerState Custom_trilinear_clamp_sampler;

shared Texture3D<float4> _VolumetricClipmapTexture;
shared Texture3D<float4> _VolumetricClipmapTexture2;
shared float3 _ClipmapPosition;
shared float _ClipmapScale;
shared float _ClipmapScale2;

shared float SeqOffset; //z offseter
float reprojectionAmount;
shared float _StaticLightMultiplier;

//float3 GetViewUpDir()
//{
//	float4x4 viewMat = PrevViewProjMatrix; //GetWorldToViewMatrix();
//	return viewMat[1].xyz;
//}
//
//float3 GetViewForwardDir()
//{
//	float4x4 viewMat = GetWorldToViewMatrix(); // UNITY_MATRIX_V
//	return -viewMat[2].xyz;
//}

//Custom trilinear interpolation
//Apparently you can't use a sampler with a RWtexture, but you can directly read the data. 
//Do we need this? Can I do this in a sampler instead without requiring a CPU memory copy? Is it any faster to do it that way? //DOES IT WORK IN ANDROID???
float4 TrilinearInterpolation(float3 UVW) {

	float3 UVW_0 = floor(UVW); //Lowest corner 
	float3 UVW_1 = ceil(UVW);	//Highest Corner

	float3 PixelDifference = UVW - UVW_0; //vec3 distance
	float3 PixelDifference_1minus = 1 - PixelDifference; 

	//Sample ALL the points!

	float4 value_000 = Result[UVW_0.xyz];
	float4 value_100 = Result[int3(UVW_1.x, UVW_0.y, UVW_0.z)];
	float4 value_010 = Result[int3(UVW_0.x, UVW_1.y, UVW_0.z)];
	float4 value_110 = Result[int3(UVW_1.x, UVW_1.y, UVW_0.z)];

	float4 value_001 = Result[int3(UVW_0.x, UVW_0.y, UVW_1.z)];
	float4 value_101 = Result[int3(UVW_1.x, UVW_0.y, UVW_1.z)];
	float4 value_011 = Result[int3(UVW_0.x, UVW_1.y, UVW_1.z)];
	float4 value_111 = Result[UVW_1.xyz];

	// Interpolate in 3 dimensions

	float4 c00 = (value_000 * (PixelDifference_1minus.x)) + (value_100 * (PixelDifference.x));
	float4 c01 = (value_001 * (PixelDifference_1minus.x)) + (value_101 * (PixelDifference.x));
	float4 c10 = (value_010 * (PixelDifference_1minus.x)) + (value_110 * (PixelDifference.x));
	float4 c11 = (value_011 * (PixelDifference_1minus.x)) + (value_111 * (PixelDifference.x));

	float4 c0 = (c00 * (PixelDifference_1minus.y)) + (c10 * (PixelDifference.y));
	float4 c1 = (c01 * (PixelDifference_1minus.y)) + (c11 * (PixelDifference.y));

	return (c0 * PixelDifference_1minus.z) + (c1 * PixelDifference.z);
};


//Realtime spot evaluation
float4 SpotLightLoop(float4 WS_coordinate) {

	float4 accumLighting = float4(0,0,0,0);

	uint count, stride;
	LightObjects.GetDimensions(count, stride);

	for (int i = 0; i < count; i++) {

		///Realtime Spotlight projection matrix and cookie sampler
		float4 lightWSPos = WS_coordinate - float4(LightObjects[i].LightPosition, 1); //world pos

		lightWSPos = mul(LightObjects[i].LightProjectionMatrix, lightWSPos);

		lightWSPos.xy = lightWSPos.xy / lightWSPos.w;

		float lightDirection = lightWSPos.z;

		lightWSPos.z = 1; //Setting which slice to sample https://docs.microsoft.com/en-us/windows/win32/direct3dhlsl/dx-graphics-hlsl-to-samplelevel

		float4 spotlightTex = LightProjectionTextureArray.SampleLevel(Custom_trilinear_clamp_sampler, lightWSPos.xyz, 0, 0);

		float LightRadius = distance(WS_coordinate.xyz, LightObjects[i].LightPosition.xyz); // Distance from light source

		spotlightTex *= LightObjects[i].LightColor / (PI_x4 *  LightRadius*LightRadius);	// inverse square

		accumLighting += spotlightTex * step(0,-lightDirection); // clip only to the front and add to the lightinbg
	}	

	return accumLighting;
}



///Main scattering pass. Intergration is done seperately so we can reproject this result for XR rather than do it twice.
[numthreads(4,4,4)]
void Scatter(uint3 id : SV_DispatchThreadID)
{

	///Get current RT resolution and convert it to 0 - 1 UVW space
	float3 whd;
	Result.GetDimensions(whd.x, whd.y, whd.z);

//	float3 TemporalOffset = float3(0.5, 0.5, 0.5 + Jittery  * 0.5); //offsetting  
	float3 TemporalOffset = float3(0.5, 0.5, 0.5 ); //offsetting  
	float4 UVW = float4( (id ) / whd, 1); //Make uvs and sample from center of froxel //ID TO UVW
	float e1 = (id.z + 1) / whd.z; // (slice + 1) / sliceCount //Todo: Bake Rcp 
//	float t1 = DecodeLogarithmicDepthGeneralized(e1, _VBufferDistanceDecodingParams);

	// UVW.z = t1;
	//TODO: replace this with the global baked bluenoise texture used elsewhere
	// Perform per-pixel randomization by adding an offset and then sampling uniformly
// (in the log space) in a vein similar to Stochastic Universal Sampling:
// https://en.wikipedia.org/wiki/Stochastic_universal_sampling
	float perPixelRandomOffset = GenerateHashedRandomFloat(id); //posInput.positionSS

	// This is a time-based sequence of 7 equidistant numbers from 1/14 to 13/14.
	// Each of them is the centroid of the interval of length 2/14.
	float rndVal = frac(perPixelRandomOffset + SeqOffset);


		/// Invert the assumed perspective projection from the UVW to World Space
	//float4 WS_coordinate = mul(CameraProjectionMatrix, UVW); //inverse camera matrix
	//WS_coordinate.xyz = WS_coordinate.xyz / WS_coordinate.w;	//tapper coord and flip around
	//WS_coordinate += CameraPosition;
	float2 centerCoord = id.xy + float2(0.5, 0.5);

	// Compute a ray direction s.t. ViewSpace(rayDirWS).z = 1.
	//float3 rayDirWS = mul(-float4(centerCoord, 1, 1), _VBufferCoordToViewDirWS[unity_StereoEyeIndex]).xyz;
	float3 rayDirWS = normalize(mul(-float4(centerCoord , 1, 1), _VBufferCoordToViewDirWS)).xyz ;
	float  rcpLenRayDir = rsqrt(dot(rayDirWS, rayDirWS));
	//rayDirWS *= rcpLenRayDir;

	//JitteredRay ray;

	//ray.originWS = GetCurrentViewPosition(); //WS pos of camera 
	//ray.centerDirWS = rayDirWS * rcpLenRayDir; // Normalize
	//float originWS = GetCurrentViewPosition(); //WS pos of camera 
//	float3 originWS = CameraPosition.xyz; //WS pos of camera 
	float3 centerDirWS = rayDirWS ; // Normalize

	//float3 F = GetViewForwardDir();
	//float3 U = GetViewUpDir();

	//float3 rightDirWS = cross(rayDirWS, U);
	//float  rcpLenRightDir = rsqrt(dot(rightDirWS, rightDirWS));

//	ray.jitterDirWS = ray.centerDirWS;
//	float de = 1 / whd.z;
//	float e1 = (UVW.z + 1) / whd.z; //linearZ
	//float  t = DecodeLogarithmicDepthGeneralized(e1 - 0.5 * de, _VBufferDistanceDecodingParams);
	float  t = DecodeLogarithmicDepthGeneralized(e1 - (rndVal * (1/whd.z) ), _VBufferDistanceDecodingParams); //Get log encoded distance based on linear UVWs
//jittered ray
	float3 centerWS = CameraPosition.xyz + (t * centerDirWS); //Cast ray along direction and add cam pos to get WS pos

	float4 accumLighting = float4(0,0,0,0);
	//	
	//centerWS *= .99;

	//WS_coordinate = -WS_coordinate + CameraPosition; //move to postion.
	float4 WS_coordinate = float4(centerWS,1);
	///
//	WS_coordinate.z += Noised;
	/// Previous frame space


////
//REPROJECTION
///

	//float4 PreviousFrameProjection = WS_coordinate; // Catch WS relitive to the camera
	//PreviousFrameProjection.xyz -= CameraMotionVector.xyz; //Move back previous frame's pos TODO: Add to matrix instead
	//PreviousFrameProjection.w = 1;
	//PreviousFrameProjection = mul(PreviousFrameMatrix, PreviousFrameProjection); // convert using previous matrix
	//PreviousFrameProjection = PreviousFrameProjection /  PreviousFrameProjection.w; //Untapper coord
/////
//	PreviousFrameProjection.z = PreviousFrameProjection.z / (_VolumePlaneSettings.y - PreviousFrameProjection.z * _VolumePlaneSettings.z);
//	PreviousFrameProjection.xy = (PreviousFrameProjection.xy + 1.0f) * 0.5f;


	//float t0 = DecodeLogarithmicDepthGeneralized(0, _VBufferDistanceDecodingParams);
	//	float de = _VBufferRcpSliceCount; // Log-encoded distance between slices

	//float4 previousResult = TrilinearInterpolation( PreviousFrameProjection.xyz * whd.xyz - 0.5); //Custom trilinear interpolation of RW texture. can't use this on mobile because can't read a RW tex. 
	//float4 previousResult = PreviousFrameLighting.SampleLevel(Custom_trilinear_clamp_sampler, id / UVW, 0 );

	//half4 ls = half4(WS_coordinate - prevPos, -1); //_WorldSpaceCameraPos

	//ls = mul(ls, transpose(PreviousFrameMatrix));
	//ls.xyz = ls.xyz / ls.w;

	float3 prevPos = CameraPosition - CameraMotionVector; //just cache this and send it rather than calulating here
	float3 ws_repro = WS_coordinate.xyz ;

	float2	positionNDC = ComputeNormalizedDeviceCoordinates(ws_repro, _PrevViewProjMatrix);
	float	vdistance = distance(ws_repro, prevPos);
	float	W = EncodeLogarithmicDepthGeneralized(vdistance, _VBufferDistanceEncodingParams);
//	float	W = vdistance;

	//half4 ls = half4(WS_coordinate - prevPos, -1); //_WorldSpaceCameraPos
	//ls = mul(ls, transpose(PreviousFrameMatrix));
	//ls.xyz = ls.xyz / ls.w;
	//float3 reprojection = float3(ls.xy, W);

	float3 reprojection = float3(positionNDC, W);

	float4 previousResult = PreviousFrameLighting.SampleLevel(Custom_trilinear_clamp_sampler, reprojection, 0 );
	//float4 previousResult = PreviousFrameLighting[id];

	/////
	//Light loops
	/////
	//	accumLighting += SpotLightLoop(WS_coordinate);

	/////
	//Baked light volumes
	/////

	//Using combined clipmap instead of direct baked maps. Reduces the amount of 3d texture look-ups per frame.
	//Sample based on worldspace divided by texture's world size. (apply to 0-1 Tex Sampler, no need to know the resolution)	
	float3 ClipmapUVW =  (WS_coordinate.xyz - (_ClipmapPosition.xyz - (_ClipmapScale *.5) )  ) / _ClipmapScale.xxx;
	
	float3 LargeClipmapUVW = (WS_coordinate.xyz - (_ClipmapPosition.xyz - (_ClipmapScale * 5 *.5) )  ) / (_ClipmapScale.xxx * 5);

	//For clipping the clipmap. Isn't needed if the view volume is smaller than the clipmap
	float Clipped =
		step(ClipmapUVW.x, 1) * step(0, ClipmapUVW.x) *
		step(ClipmapUVW.y, 1) * step(0, ClipmapUVW.y) *
		step(ClipmapUVW.z, 1) * step(0, ClipmapUVW.z);
	Clipped = saturate(Clipped);

	//if (Clipped > 0) { //Android doesn't like ifs :/
	//	accumLighting += float4(_VolumetricClipmapTexture.SampleLevel(Custom_trilinear_clamp_sampler, ClipmapUVW, 0).xyz, 0);
	//}
	//else
	//{
	//	accumLighting += float4(_VolumetricClipmapTexture2.SampleLevel(Custom_trilinear_clamp_sampler, LargeClipmapUVW, 0).xyz, 0);
	//}
	accumLighting += float4(_VolumetricClipmapTexture.SampleLevel(Custom_trilinear_clamp_sampler, ClipmapUVW, 0).xyz, 0) * Clipped;
	accumLighting += float4(_VolumetricClipmapTexture2.SampleLevel(Custom_trilinear_clamp_sampler, LargeClipmapUVW, 0).xyz, 0) * (1- Clipped);

	accumLighting *= _StaticLightMultiplier * _GlobalExtinction;

//	accumLighting.rgb += max((float3(rndVal, frac(rndVal+.2), frac(rndVal+.5)) - 0.5) * 0.03,0);  //dithering

	Result[id.xyz] = lerp(accumLighting , previousResult, reprojectionAmount ); // Temporal sampling 

}
